{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac1414d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Country</th>\n",
       "      <th>Race</th>\n",
       "      <th>Family_Background</th>\n",
       "      <th>Radiation_History</th>\n",
       "      <th>Iodine_Deficiency</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Weight_Risk</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Nodule_Size</th>\n",
       "      <th>TSH_Result</th>\n",
       "      <th>T4_Result</th>\n",
       "      <th>T3_Result</th>\n",
       "      <th>Cancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>80</td>\n",
       "      <td>M</td>\n",
       "      <td>CHN</td>\n",
       "      <td>ASN</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Exposed</td>\n",
       "      <td>Sufficient</td>\n",
       "      <td>Non-Smoker</td>\n",
       "      <td>Not Obese</td>\n",
       "      <td>No</td>\n",
       "      <td>0.650355</td>\n",
       "      <td>2.784735</td>\n",
       "      <td>6.744603</td>\n",
       "      <td>2.575820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>NGA</td>\n",
       "      <td>ASN</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Unexposed</td>\n",
       "      <td>Sufficient</td>\n",
       "      <td>Smoker</td>\n",
       "      <td>Obese</td>\n",
       "      <td>No</td>\n",
       "      <td>2.950430</td>\n",
       "      <td>0.911624</td>\n",
       "      <td>7.303305</td>\n",
       "      <td>2.505317</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>71</td>\n",
       "      <td>M</td>\n",
       "      <td>CHN</td>\n",
       "      <td>MDE</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Unexposed</td>\n",
       "      <td>Sufficient</td>\n",
       "      <td>Non-Smoker</td>\n",
       "      <td>Not Obese</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2.200023</td>\n",
       "      <td>0.717754</td>\n",
       "      <td>11.137459</td>\n",
       "      <td>2.381080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>40</td>\n",
       "      <td>F</td>\n",
       "      <td>IND</td>\n",
       "      <td>HSP</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Unexposed</td>\n",
       "      <td>Sufficient</td>\n",
       "      <td>Non-Smoker</td>\n",
       "      <td>Obese</td>\n",
       "      <td>No</td>\n",
       "      <td>3.370796</td>\n",
       "      <td>6.846380</td>\n",
       "      <td>10.175254</td>\n",
       "      <td>0.753023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>CHN</td>\n",
       "      <td>CAU</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Unexposed</td>\n",
       "      <td>Sufficient</td>\n",
       "      <td>Non-Smoker</td>\n",
       "      <td>Not Obese</td>\n",
       "      <td>No</td>\n",
       "      <td>4.230048</td>\n",
       "      <td>0.439519</td>\n",
       "      <td>7.194450</td>\n",
       "      <td>0.569356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  Age Gender Country Race Family_Background Radiation_History  \\\n",
       "0  TRAIN_00000   80      M     CHN  ASN          Positive           Exposed   \n",
       "1  TRAIN_00001   37      M     NGA  ASN          Positive         Unexposed   \n",
       "2  TRAIN_00002   71      M     CHN  MDE          Positive         Unexposed   \n",
       "3  TRAIN_00003   40      F     IND  HSP          Negative         Unexposed   \n",
       "4  TRAIN_00004   53      F     CHN  CAU          Negative         Unexposed   \n",
       "\n",
       "  Iodine_Deficiency       Smoke Weight_Risk Diabetes  Nodule_Size  TSH_Result  \\\n",
       "0        Sufficient  Non-Smoker   Not Obese       No     0.650355    2.784735   \n",
       "1        Sufficient      Smoker       Obese       No     2.950430    0.911624   \n",
       "2        Sufficient  Non-Smoker   Not Obese      Yes     2.200023    0.717754   \n",
       "3        Sufficient  Non-Smoker       Obese       No     3.370796    6.846380   \n",
       "4        Sufficient  Non-Smoker   Not Obese       No     4.230048    0.439519   \n",
       "\n",
       "   T4_Result  T3_Result  Cancer  \n",
       "0   6.744603   2.575820       1  \n",
       "1   7.303305   2.505317       1  \n",
       "2  11.137459   2.381080       0  \n",
       "3  10.175254   0.753023       0  \n",
       "4   7.194450   0.569356       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 학습 데이터 로드\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# 상위 5개 행 확인\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca67e7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Country</th>\n",
       "      <th>Race</th>\n",
       "      <th>Family_Background</th>\n",
       "      <th>Radiation_History</th>\n",
       "      <th>Iodine_Deficiency</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Weight_Risk</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Nodule_Size</th>\n",
       "      <th>TSH_Result</th>\n",
       "      <th>T4_Result</th>\n",
       "      <th>T3_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>53</td>\n",
       "      <td>M</td>\n",
       "      <td>NGA</td>\n",
       "      <td>CAU</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Unexposed</td>\n",
       "      <td>Sufficient</td>\n",
       "      <td>Non-Smoker</td>\n",
       "      <td>Not Obese</td>\n",
       "      <td>No</td>\n",
       "      <td>2.940678</td>\n",
       "      <td>6.434501</td>\n",
       "      <td>9.227958</td>\n",
       "      <td>2.733846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>46</td>\n",
       "      <td>M</td>\n",
       "      <td>KOR</td>\n",
       "      <td>MDE</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Unexposed</td>\n",
       "      <td>Deficient</td>\n",
       "      <td>Smoker</td>\n",
       "      <td>Obese</td>\n",
       "      <td>No</td>\n",
       "      <td>3.450129</td>\n",
       "      <td>4.729535</td>\n",
       "      <td>5.937664</td>\n",
       "      <td>0.775012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>78</td>\n",
       "      <td>F</td>\n",
       "      <td>IND</td>\n",
       "      <td>ASN</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Exposed</td>\n",
       "      <td>Sufficient</td>\n",
       "      <td>Non-Smoker</td>\n",
       "      <td>Obese</td>\n",
       "      <td>No</td>\n",
       "      <td>4.680720</td>\n",
       "      <td>5.663475</td>\n",
       "      <td>10.455964</td>\n",
       "      <td>1.259896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>69</td>\n",
       "      <td>F</td>\n",
       "      <td>KOR</td>\n",
       "      <td>CAU</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Unexposed</td>\n",
       "      <td>Sufficient</td>\n",
       "      <td>Non-Smoker</td>\n",
       "      <td>Not Obese</td>\n",
       "      <td>No</td>\n",
       "      <td>4.280588</td>\n",
       "      <td>7.473752</td>\n",
       "      <td>8.785335</td>\n",
       "      <td>2.826138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>77</td>\n",
       "      <td>F</td>\n",
       "      <td>KOR</td>\n",
       "      <td>CAU</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Unexposed</td>\n",
       "      <td>Sufficient</td>\n",
       "      <td>Non-Smoker</td>\n",
       "      <td>Obese</td>\n",
       "      <td>No</td>\n",
       "      <td>3.380094</td>\n",
       "      <td>7.099987</td>\n",
       "      <td>8.737063</td>\n",
       "      <td>3.453563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Age Gender Country Race Family_Background Radiation_History  \\\n",
       "0  TEST_00000   53      M     NGA  CAU          Negative         Unexposed   \n",
       "1  TEST_00001   46      M     KOR  MDE          Negative         Unexposed   \n",
       "2  TEST_00002   78      F     IND  ASN          Negative           Exposed   \n",
       "3  TEST_00003   69      F     KOR  CAU          Negative         Unexposed   \n",
       "4  TEST_00004   77      F     KOR  CAU          Positive         Unexposed   \n",
       "\n",
       "  Iodine_Deficiency       Smoke Weight_Risk Diabetes  Nodule_Size  TSH_Result  \\\n",
       "0        Sufficient  Non-Smoker   Not Obese       No     2.940678    6.434501   \n",
       "1         Deficient      Smoker       Obese       No     3.450129    4.729535   \n",
       "2        Sufficient  Non-Smoker       Obese       No     4.680720    5.663475   \n",
       "3        Sufficient  Non-Smoker   Not Obese       No     4.280588    7.473752   \n",
       "4        Sufficient  Non-Smoker       Obese       No     3.380094    7.099987   \n",
       "\n",
       "   T4_Result  T3_Result  \n",
       "0   9.227958   2.733846  \n",
       "1   5.937664   0.775012  \n",
       "2  10.455964   1.259896  \n",
       "3   8.785335   2.826138  \n",
       "4   8.737063   3.453563  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 학습 데이터 로드\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# 상위 5개 행 확인\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649513d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1122\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
      "[LightGBM] [Info] Start training from score -1.992463\n",
      "Validation F1 Score: 0.2901\n"
     ]
    }
   ],
   "source": [
    "# cancer_detection_pipeline.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 2. 전처리\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    label_cols = df.select_dtypes(include='object').columns\n",
    "    le_dict = {}\n",
    "    for col in label_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        le_dict[col] = le\n",
    "    return df, le_dict\n",
    "\n",
    "train_x = train.drop(['ID', 'Cancer'], axis=1)\n",
    "train_y = train['Cancer']\n",
    "train_x, label_encoders = preprocess(train_x)\n",
    "\n",
    "# 테스트 데이터에도 동일한 전처리\n",
    "test_ids = test['ID']\n",
    "test_x = test.drop(['ID'], axis=1)\n",
    "for col, le in label_encoders.items():\n",
    "    test_x[col] = le.transform(test_x[col])\n",
    "\n",
    "# 3. 습/검증 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=42, stratify=train_y\n",
    ")\n",
    "\n",
    "# 4. 모델 학습\n",
    "model = LGBMClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. 검증\n",
    "val_preds = model.predict(X_val)\n",
    "f1 = f1_score(y_val, val_preds)\n",
    "print(\"Validation F1 Score:\", round(f1, 4))\n",
    "\n",
    "# 6. 테스트 데이터 예측 및 제출 파일 저장\n",
    "test_preds = model.predict(test_x)\n",
    "submission['Cancer'] = test_preds\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c26b389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1122\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.55331\tvalid_0's f1: 0.466397\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.559417\tvalid_0's f1: 0.466869\n",
      "Validation F1 Score: 0.4669\n"
     ]
    }
   ],
   "source": [
    "# cancer_detection_pipeline.py\n",
    "# Private 제출용 코드\n",
    "# 모델: LGBMClassifier + class_weight + F1 최적화\n",
    "# 경로: /data 기준\n",
    "# 개발환경: Python 3.9, LightGBM 4.2.0, scikit-learn 1.3.0\n",
    "# 코드 인코딩: UTF-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 2. 전처리 함수 정의\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    label_cols = df.select_dtypes(include='object').columns\n",
    "    le_dict = {}\n",
    "    for col in label_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        le_dict[col] = le\n",
    "    return df, le_dict\n",
    "\n",
    "# 3. 훈련 데이터 준비\n",
    "train_x = train.drop(['ID', 'Cancer'], axis=1)\n",
    "train_y = train['Cancer']\n",
    "train_x, label_encoders = preprocess(train_x)\n",
    "\n",
    "# 4. 테스트 데이터 전처리\n",
    "test_ids = test['ID']\n",
    "test_x = test.drop(['ID'], axis=1)\n",
    "for col, le in label_encoders.items():\n",
    "    test_x[col] = le.transform(test_x[col])\n",
    "\n",
    "# 5. 학습/검증 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=42, stratify=train_y\n",
    ")\n",
    "\n",
    "# 6. 커스텀 F1 스코어 함수 정의\n",
    "def lgb_f1_score(y_true, y_pred):\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    return 'f1', f1_score(y_true, y_pred_binary), True\n",
    "\n",
    "# 7. 모델 학습\n",
    "model = LGBMClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced',         # 클래스 불균형 대응\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=lgb_f1_score,\n",
    "    callbacks=[\n",
    "        early_stopping(50),\n",
    "        log_evaluation(100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# 8. 검증 점수 출력\n",
    "val_preds = model.predict(X_val)\n",
    "val_f1 = f1_score(y_val, val_preds)\n",
    "print(\"Validation F1 Score:\", round(val_f1, 4))\n",
    "\n",
    "# 9. 테스트 데이터 예측 및 제출\n",
    "test_preds = model.predict(test_x)\n",
    "submission['Cancer'] = test_preds\n",
    "submission.to_csv('submission2.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca814a12",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: Gender: object, Country: object, Race: object, Family_Background: object, Radiation_History: object, Iodine_Deficiency: object, Smoke: object, Weight_Risk: object, Diabetes: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 57\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# 8. 모델 학습\u001b[39;00m\n\u001b[0;32m     49\u001b[0m model \u001b[38;5;241m=\u001b[39m LGBMClassifier(\n\u001b[0;32m     50\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     51\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     55\u001b[0m )\n\u001b[1;32m---> 57\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     58\u001b[0m     X_train, y_train,\n\u001b[0;32m     59\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39m[(X_val, y_val)],\n\u001b[0;32m     60\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39mlgb_f1_score,\n\u001b[0;32m     61\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping(\u001b[38;5;241m50\u001b[39m), log_evaluation(\u001b[38;5;241m100\u001b[39m)],\n\u001b[0;32m     62\u001b[0m     categorical_feature\u001b[38;5;241m=\u001b[39mcat_cols\n\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# 9. Threshold 튜닝을 통한 F1 최적화\u001b[39;00m\n\u001b[0;32m     66\u001b[0m val_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:1560\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1558\u001b[0m             valid_sets\u001b[38;5;241m.\u001b[39mappend((valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y)))\n\u001b[1;32m-> 1560\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m   1561\u001b[0m     X,\n\u001b[0;32m   1562\u001b[0m     _y,\n\u001b[0;32m   1563\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1564\u001b[0m     init_score\u001b[38;5;241m=\u001b[39minit_score,\n\u001b[0;32m   1565\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39mvalid_sets,\n\u001b[0;32m   1566\u001b[0m     eval_names\u001b[38;5;241m=\u001b[39meval_names,\n\u001b[0;32m   1567\u001b[0m     eval_sample_weight\u001b[38;5;241m=\u001b[39meval_sample_weight,\n\u001b[0;32m   1568\u001b[0m     eval_class_weight\u001b[38;5;241m=\u001b[39meval_class_weight,\n\u001b[0;32m   1569\u001b[0m     eval_init_score\u001b[38;5;241m=\u001b[39meval_init_score,\n\u001b[0;32m   1570\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39meval_metric,\n\u001b[0;32m   1571\u001b[0m     feature_name\u001b[38;5;241m=\u001b[39mfeature_name,\n\u001b[0;32m   1572\u001b[0m     categorical_feature\u001b[38;5;241m=\u001b[39mcategorical_feature,\n\u001b[0;32m   1573\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m   1574\u001b[0m     init_model\u001b[38;5;241m=\u001b[39minit_model,\n\u001b[0;32m   1575\u001b[0m )\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1046\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1047\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1050\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m   1051\u001b[0m     train_set\u001b[38;5;241m=\u001b[39mtrain_set,\n\u001b[0;32m   1052\u001b[0m     num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators,\n\u001b[0;32m   1053\u001b[0m     valid_sets\u001b[38;5;241m=\u001b[39mvalid_sets,\n\u001b[0;32m   1054\u001b[0m     valid_names\u001b[38;5;241m=\u001b[39meval_names,\n\u001b[0;32m   1055\u001b[0m     feval\u001b[38;5;241m=\u001b[39meval_metrics_callable,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     init_model\u001b[38;5;241m=\u001b[39minit_model,\n\u001b[0;32m   1057\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m   1058\u001b[0m )\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mnum_feature()\n",
      "File \u001b[1;32mc:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\lightgbm\\engine.py:297\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     booster \u001b[38;5;241m=\u001b[39m Booster(params\u001b[38;5;241m=\u001b[39mparams, train_set\u001b[38;5;241m=\u001b[39mtrain_set)\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    299\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32mc:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:3656\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[0;32m   3649\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[0;32m   3650\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[0;32m   3651\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   3652\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[0;32m   3653\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   3654\u001b[0m     )\n\u001b[0;32m   3655\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[1;32m-> 3656\u001b[0m train_set\u001b[38;5;241m.\u001b[39mconstruct()\n\u001b[0;32m   3657\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[1;32mc:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:2590\u001b[0m, in \u001b[0;36mDataset.construct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2585\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\n\u001b[0;32m   2586\u001b[0m                 predictor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices\u001b[38;5;241m=\u001b[39mused_indices\n\u001b[0;32m   2587\u001b[0m             )\n\u001b[0;32m   2588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2589\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[1;32m-> 2590\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_init(\n\u001b[0;32m   2591\u001b[0m         data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m   2592\u001b[0m         label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel,\n\u001b[0;32m   2593\u001b[0m         reference\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2594\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   2595\u001b[0m         group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup,\n\u001b[0;32m   2596\u001b[0m         init_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_score,\n\u001b[0;32m   2597\u001b[0m         predictor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor,\n\u001b[0;32m   2598\u001b[0m         feature_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_name,\n\u001b[0;32m   2599\u001b[0m         categorical_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_feature,\n\u001b[0;32m   2600\u001b[0m         params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams,\n\u001b[0;32m   2601\u001b[0m         position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition,\n\u001b[0;32m   2602\u001b[0m     )\n\u001b[0;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[0;32m   2604\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:2123\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params, position)\u001b[0m\n\u001b[0;32m   2121\u001b[0m     categorical_feature \u001b[38;5;241m=\u001b[39m reference\u001b[38;5;241m.\u001b[39mcategorical_feature\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[1;32m-> 2123\u001b[0m     data, feature_name, categorical_feature, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical \u001b[38;5;241m=\u001b[39m _data_from_pandas(\n\u001b[0;32m   2124\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m   2125\u001b[0m         feature_name\u001b[38;5;241m=\u001b[39mfeature_name,\n\u001b[0;32m   2126\u001b[0m         categorical_feature\u001b[38;5;241m=\u001b[39mcategorical_feature,\n\u001b[0;32m   2127\u001b[0m         pandas_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical,\n\u001b[0;32m   2128\u001b[0m     )\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data) \u001b[38;5;129;01mand\u001b[39;00m feature_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2130\u001b[0m     feature_name \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumn_names\n",
      "File \u001b[1;32mc:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:868\u001b[0m, in \u001b[0;36m_data_from_pandas\u001b[1;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[0;32m    864\u001b[0m df_dtypes\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    865\u001b[0m target_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdf_dtypes)\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 868\u001b[0m     _pandas_to_numpy(data, target_dtype\u001b[38;5;241m=\u001b[39mtarget_dtype),\n\u001b[0;32m    869\u001b[0m     feature_name,\n\u001b[0;32m    870\u001b[0m     categorical_feature,\n\u001b[0;32m    871\u001b[0m     pandas_categorical,\n\u001b[0;32m    872\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:814\u001b[0m, in \u001b[0;36m_pandas_to_numpy\u001b[1;34m(data, target_dtype)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pandas_to_numpy\u001b[39m(\n\u001b[0;32m    811\u001b[0m     data: pd_DataFrame,\n\u001b[0;32m    812\u001b[0m     target_dtype: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.typing.DTypeLike\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 814\u001b[0m     _check_for_bad_pandas_dtypes(data\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;66;03m# most common case (no nullable dtypes)\u001b[39;00m\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:805\u001b[0m, in \u001b[0;36m_check_for_bad_pandas_dtypes\u001b[1;34m(pandas_dtypes_series)\u001b[0m\n\u001b[0;32m    799\u001b[0m bad_pandas_dtypes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpandas_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column_name, pandas_dtype \u001b[38;5;129;01min\u001b[39;00m pandas_dtypes_series\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_allowed_numpy_dtype(pandas_dtype\u001b[38;5;241m.\u001b[39mtype)\n\u001b[0;32m    803\u001b[0m ]\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bad_pandas_dtypes:\n\u001b[1;32m--> 805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas dtypes must be int, float or bool.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFields with bad pandas dtypes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(bad_pandas_dtypes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    807\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: Gender: object, Country: object, Race: object, Family_Background: object, Radiation_History: object, Iodine_Deficiency: object, Smoke: object, Weight_Risk: object, Diabetes: object"
     ]
    }
   ],
   "source": [
    "# cancer_detection_pipeline_optimized.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "import os\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 2. 전처리 + Feature Engineering 함수 정의\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 수치형 Feature Engineering 예시\n",
    "    df['TSH_T3_ratio'] = df['TSH_Result'] / (df['T3_Result'] + 1e-6)\n",
    "    df['T4_T3_ratio'] = df['T4_Result'] / (df['T3_Result'] + 1e-6)\n",
    "    df['Nodule_TSH_ratio'] = df['Nodule_Size'] / (df['TSH_Result'] + 1e-6)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 3. 훈련 데이터 준비\n",
    "train_y = train['Cancer']\n",
    "train_x = train.drop(['ID', 'Cancer'], axis=1)\n",
    "train_x = preprocess(train_x)\n",
    "\n",
    "# 4. 테스트 데이터 전처리\n",
    "test_ids = test['ID']\n",
    "test_x = test.drop(['ID'], axis=1)\n",
    "test_x = preprocess(test_x)\n",
    "\n",
    "# 5. 카테고리형 컬럼 지정 (문자형인 열들)\n",
    "cat_cols = train_x.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# 6. 학습/검증 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=42, stratify=train_y\n",
    ")\n",
    "\n",
    "# 7. 커스텀 F1 스코어 함수 정의\n",
    "def lgb_f1_score(y_true, y_pred):\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    return 'f1', f1_score(y_true, y_pred_binary), True\n",
    "\n",
    "# 8. 모델 학습\n",
    "model = LGBMClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=lgb_f1_score,\n",
    "    callbacks=[early_stopping(50), log_evaluation(100)],\n",
    "    categorical_feature=cat_cols\n",
    ")\n",
    "\n",
    "# 9. Threshold 튜닝을 통한 F1 최적화\n",
    "val_probs = model.predict_proba(X_val)[:, 1]\n",
    "prec, rec, thresholds = precision_recall_curve(y_val, val_probs)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-6)\n",
    "best_thresh = thresholds[np.argmax(f1s)]\n",
    "print(f\"Best Threshold: {best_thresh:.4f}\")\n",
    "\n",
    "val_preds = (val_probs > best_thresh).astype(int)\n",
    "val_f1 = f1_score(y_val, val_preds)\n",
    "print(\"Optimized Validation F1 Score:\", round(val_f1, 4))\n",
    "\n",
    "# 10. 테스트 데이터 예측 및 제출 파일 생성\n",
    "test_probs = model.predict_proba(test_x)[:, 1]\n",
    "test_preds = (test_probs > best_thresh).astype(int)\n",
    "submission['Cancer'] = test_preds\n",
    "submission.to_csv('submission4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcf13a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1896\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.551795\tvalid_0's f1: 0.466465\n",
      "Early stopping, best iteration is:\n",
      "[54]\tvalid_0's binary_logloss: 0.558458\tvalid_0's f1: 0.467881\n",
      "Best Threshold: 0.7111\n",
      "Optimized Validation F1 Score: 0.4676\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "import os\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 2. 전처리 + Feature Engineering 함수 정의\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 수치형 Feature Engineering\n",
    "    df['TSH_T3_ratio'] = df['TSH_Result'] / (df['T3_Result'] + 1e-6)\n",
    "    df['T4_T3_ratio'] = df['T4_Result'] / (df['T3_Result'] + 1e-6)\n",
    "    df['Nodule_TSH_ratio'] = df['Nodule_Size'] / (df['TSH_Result'] + 1e-6)\n",
    "\n",
    "    # 범주형 변수 object -> category 변환\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "# 3. 훈련 데이터 준비\n",
    "train_y = train['Cancer']\n",
    "train_x = train.drop(['ID', 'Cancer'], axis=1)\n",
    "train_x = preprocess(train_x)\n",
    "\n",
    "# 4. 테스트 데이터 전처리\n",
    "test_ids = test['ID']\n",
    "test_x = test.drop(['ID'], axis=1)\n",
    "test_x = preprocess(test_x)\n",
    "\n",
    "# 5. 카테고리형 컬럼 추출\n",
    "cat_cols = train_x.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "# 6. 학습/검증 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=42, stratify=train_y\n",
    ")\n",
    "\n",
    "# 7. 커스텀 F1 스코어 함수 정의\n",
    "def lgb_f1_score(y_true, y_pred):\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    return 'f1', f1_score(y_true, y_pred_binary), True\n",
    "\n",
    "# 8. 모델 정의 및 학습\n",
    "model = LGBMClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=lgb_f1_score,\n",
    "    callbacks=[early_stopping(50), log_evaluation(100)],\n",
    "    categorical_feature=cat_cols\n",
    ")\n",
    "\n",
    "# 9. Threshold 튜닝을 통한 F1 최적화\n",
    "val_probs = model.predict_proba(X_val)[:, 1]\n",
    "prec, rec, thresholds = precision_recall_curve(y_val, val_probs)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-6)\n",
    "best_thresh = thresholds[np.argmax(f1s)]\n",
    "print(f\"Best Threshold: {best_thresh:.4f}\")\n",
    "\n",
    "val_preds = (val_probs > best_thresh).astype(int)\n",
    "val_f1 = f1_score(y_val, val_preds)\n",
    "print(\"Optimized Validation F1 Score:\", round(val_f1, 4))\n",
    "\n",
    "# 10. 테스트 데이터 예측 및 제출 파일 생성\n",
    "test_probs = model.predict_proba(test_x)[:, 1]\n",
    "test_preds = (test_probs > best_thresh).astype(int)\n",
    "\n",
    "submission['Cancer'] = test_preds\n",
    "submission.to_csv('submission4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65221332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2151\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Best parameters found: {'learning_rate': 0.01, 'max_depth': 20, 'n_estimators': 1000, 'num_leaves': 31}\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2151\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.585358\tvalid_0's f1: 0.466633\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid_0's binary_logloss: 0.573839\tvalid_0's f1: 0.467223\n",
      "Best Threshold: 0.7041\n",
      "Optimized Validation F1 Score: 0.4675\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "import os\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 2. 전처리 + Feature Engineering 함수 정의\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 수치형 Feature Engineering\n",
    "    df['TSH_T3_ratio'] = df['TSH_Result'] / (df['T3_Result'] + 1e-6)\n",
    "    df['T4_T3_ratio'] = df['T4_Result'] / (df['T3_Result'] + 1e-6)\n",
    "    df['Nodule_TSH_ratio'] = df['Nodule_Size'] / (df['TSH_Result'] + 1e-6)\n",
    "    df['TSH_T4_ratio'] = df['TSH_Result'] / (df['T4_Result'] + 1e-6)  # 추가 피처 엔지니어링\n",
    "\n",
    "    # 범주형 변수 object -> category 변환\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "# 3. 훈련 데이터 준비\n",
    "train_y = train['Cancer']\n",
    "train_x = train.drop(['ID', 'Cancer'], axis=1)\n",
    "train_x = preprocess(train_x)\n",
    "\n",
    "# 4. 테스트 데이터 전처리\n",
    "test_ids = test['ID']\n",
    "test_x = test.drop(['ID'], axis=1)\n",
    "test_x = preprocess(test_x)\n",
    "\n",
    "# 5. 카테고리형 컬럼 추출\n",
    "cat_cols = train_x.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "# 6. 학습/검증 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=42, stratify=train_y\n",
    ")\n",
    "\n",
    "# 7. 커스텀 F1 스코어 함수 정의\n",
    "def lgb_f1_score(y_true, y_pred):\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    return 'f1', f1_score(y_true, y_pred_binary), True\n",
    "\n",
    "# 8. 모델 정의 및 하이퍼파라미터 튜닝을 위한 GridSearchCV\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [1000, 1500, 2000]\n",
    "}\n",
    "\n",
    "model = LGBMClassifier(random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='f1', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 파라미터 출력\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# 9. 최적의 파라미터로 모델 학습\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# 10. 모델 학습\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=lgb_f1_score,\n",
    "    callbacks=[early_stopping(50), log_evaluation(100)],\n",
    "    categorical_feature=cat_cols\n",
    ")\n",
    "\n",
    "# 11. Threshold 튜닝을 통한 F1 최적화\n",
    "val_probs = best_model.predict_proba(X_val)[:, 1]\n",
    "prec, rec, thresholds = precision_recall_curve(y_val, val_probs)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-6)\n",
    "best_thresh = thresholds[np.argmax(f1s)]\n",
    "print(f\"Best Threshold: {best_thresh:.4f}\")\n",
    "\n",
    "val_preds = (val_probs > best_thresh).astype(int)\n",
    "val_f1 = f1_score(y_val, val_preds)\n",
    "print(\"Optimized Validation F1 Score:\", round(val_f1, 4))\n",
    "\n",
    "# 12. 앙상블 모델 (LightGBM + 다른 모델) 적용 (Optional)\n",
    "# LightGBM과 다른 모델을 앙상블할 경우, 예를 들어 XGBoost나 다른 분류기를 추가할 수 있습니다.\n",
    "\n",
    "# 13. 테스트 데이터 예측 및 제출 파일 생성\n",
    "test_probs = best_model.predict_proba(test_x)[:, 1]\n",
    "test_preds = (test_probs > best_thresh).astype(int)\n",
    "\n",
    "submission['Cancer'] = test_preds\n",
    "submission.to_csv('submission_optimized.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "936bb7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004606 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1896\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.551795\tvalid_0's f1: 0.466465\n",
      "Early stopping, best iteration is:\n",
      "[54]\tvalid_0's binary_logloss: 0.558458\tvalid_0's f1: 0.467881\n",
      "Best Threshold: 0.7111\n",
      "Optimized Validation F1 Score: 0.4676\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "import os\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 2. 전처리 및 Feature Engineering 함수 정의\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 수치형 Feature Engineering\n",
    "    df['TSH_T3_ratio'] = df['TSH_Result'] / (df['T3_Result'] + 1e-6)  # TSH와 T3의 비율\n",
    "    df['T4_T3_ratio'] = df['T4_Result'] / (df['T3_Result'] + 1e-6)  # T4와 T3의 비율\n",
    "    df['Nodule_TSH_ratio'] = df['Nodule_Size'] / (df['TSH_Result'] + 1e-6)  # 결절 크기와 TSH의 비율\n",
    "\n",
    "    # 범주형 변수 object -> category로 변환\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "# 3. 훈련 데이터 준비\n",
    "train_y = train['Cancer']\n",
    "train_x = train.drop(['ID', 'Cancer'], axis=1)\n",
    "train_x = preprocess(train_x)\n",
    "\n",
    "# 4. 테스트 데이터 전처리\n",
    "test_ids = test['ID']\n",
    "test_x = test.drop(['ID'], axis=1)\n",
    "test_x = preprocess(test_x)\n",
    "\n",
    "# 5. 카테고리형 변수 추출\n",
    "cat_cols = train_x.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "# 6. 학습/검증 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=42, stratify=train_y\n",
    ")\n",
    "\n",
    "# 7. 커스텀 F1 스코어 함수 정의\n",
    "def lgb_f1_score(y_true, y_pred):\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)  # 예측값을 0 또는 1로 이진화\n",
    "    return 'f1', f1_score(y_true, y_pred_binary), True\n",
    "\n",
    "# 8. 모델 정의 및 학습\n",
    "model = LGBMClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced',  # 불균형 클래스에 가중치를 부여\n",
    "    n_estimators=1000,  # 트리의 개수\n",
    "    learning_rate=0.05,  # 학습률\n",
    "    n_jobs=-1  # 병렬 처리\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=lgb_f1_score,  # F1 스코어를 평가 지표로 사용\n",
    "    callbacks=[early_stopping(50), log_evaluation(100)],  # 조기 종료 및 로그 출력\n",
    "    categorical_feature=cat_cols  # 카테고리형 변수 지정\n",
    ")\n",
    "\n",
    "# 9. Threshold 튜닝을 통한 F1 최적화\n",
    "val_probs = model.predict_proba(X_val)[:, 1]  # 예측 확률 가져오기\n",
    "prec, rec, thresholds = precision_recall_curve(y_val, val_probs)  # Precision-Recall 커브\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-6)  # F1 스코어 계산\n",
    "best_thresh = thresholds[np.argmax(f1s)]  # 최적의 threshold 찾기\n",
    "print(f\"Best Threshold: {best_thresh:.4f}\")\n",
    "\n",
    "# 최적 threshold에 따른 예측\n",
    "val_preds = (val_probs > best_thresh).astype(int)\n",
    "val_f1 = f1_score(y_val, val_preds)  # 최적화된 F1 스코어 계산\n",
    "print(\"Optimized Validation F1 Score:\", round(val_f1, 4))\n",
    "\n",
    "# 10. 테스트 데이터 예측 및 제출 파일 생성\n",
    "test_probs = model.predict_proba(test_x)[:, 1]\n",
    "test_preds = (test_probs > best_thresh).astype(int)\n",
    "\n",
    "submission['Cancer'] = test_preds\n",
    "submission.to_csv('submission5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2666\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.560565\tvalid_0's f1: 0.484195\n",
      "Early stopping, best iteration is:\n",
      "[101]\tvalid_0's binary_logloss: 0.560407\tvalid_0's f1: 0.484316\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2666\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.555033\tvalid_0's f1: 0.488265\n",
      "Early stopping, best iteration is:\n",
      "[138]\tvalid_0's binary_logloss: 0.550158\tvalid_0's f1: 0.489161\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2665\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.632667\tvalid_0's f1: 0.479021\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2666\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.558844\tvalid_0's f1: 0.485364\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's binary_logloss: 0.568396\tvalid_0's f1: 0.485757\n",
      "[LightGBM] [Info] Number of positive: 8368, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2665\n",
      "[LightGBM] [Info] Number of data points in the train set: 69728, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.556953\tvalid_0's f1: 0.490033\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's binary_logloss: 0.557863\tvalid_0's f1: 0.490033\n",
      "Best Threshold (CV): 0.6005\n",
      "Best F1 Score (CV): 0.4865\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 2. 전처리 및 Feature Engineering 함수\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    # 비율 기반 Feature\n",
    "    df['TSH_T3_ratio'] = df['TSH_Result'] / (df['T3_Result'] + 1e-6)\n",
    "    df['T4_T3_ratio'] = df['T4_Result'] / (df['T3_Result'] + 1e-6)\n",
    "    df['Nodule_TSH_ratio'] = df['Nodule_Size'] / (df['TSH_Result'] + 1e-6)\n",
    "    \n",
    "    # 호르몬 수치 통계 Feature\n",
    "    df['hormone_sum'] = df['TSH_Result'] + df['T3_Result'] + df['T4_Result']\n",
    "    df['hormone_mean'] = df[['TSH_Result', 'T3_Result', 'T4_Result']].mean(axis=1)\n",
    "    df['hormone_std'] = df[['TSH_Result', 'T3_Result', 'T4_Result']].std(axis=1)\n",
    "\n",
    "    # 이상치 여부\n",
    "    df['TSH_outlier'] = (df['TSH_Result'] > df['TSH_Result'].quantile(0.95)).astype(int)\n",
    "    df['T3_outlier'] = (df['T3_Result'] < df['T3_Result'].quantile(0.05)).astype(int)\n",
    "\n",
    "    # 범주형 변환\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "# 3. 훈련 및 테스트 데이터 전처리\n",
    "train_y = train['Cancer']\n",
    "train_x = preprocess(train.drop(['ID', 'Cancer'], axis=1))\n",
    "test_ids = test['ID']\n",
    "test_x = preprocess(test.drop(['ID'], axis=1))\n",
    "\n",
    "cat_cols = train_x.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "# 4. F1 커스텀 평가 함수\n",
    "def lgb_f1_score(y_true, y_pred):\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    return 'f1', f1_score(y_true, y_pred_binary), True\n",
    "\n",
    "# 5. Stratified K-Fold 학습\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "val_probs_all = np.zeros(len(train_x))\n",
    "test_probs_all = np.zeros(len(test_x))\n",
    "\n",
    "for train_idx, val_idx in skf.split(train_x, train_y):\n",
    "    X_tr, X_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "    y_tr, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=7,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=lgb_f1_score,\n",
    "        callbacks=[early_stopping(50), log_evaluation(100)],\n",
    "        categorical_feature=cat_cols\n",
    "    )\n",
    "\n",
    "    val_probs_all[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    test_probs_all += model.predict_proba(test_x)[:, 1] / skf.n_splits\n",
    "\n",
    "# 6. 최적 Threshold 계산\n",
    "prec, rec, thresholds = precision_recall_curve(train_y, val_probs_all)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-6)\n",
    "best_thresh = thresholds[np.argmax(f1s)]\n",
    "print(f\"Best Threshold (CV): {best_thresh:.4f}\")\n",
    "print(f\"Best F1 Score (CV): {np.max(f1s):.4f}\")\n",
    "\n",
    "# 7. 예측 및 제출\n",
    "test_preds = (test_probs_all > best_thresh).astype(int)\n",
    "submission['Cancer'] = test_preds\n",
    "submission.to_csv('submission6.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df7d906c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1123\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
      "[LightGBM] [Info] Start training from score -1.992463\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.308741\n",
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's binary_logloss: 0.308061\n",
      "Best Threshold (Fold): 0.3000, Best F1: 0.4814\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1122\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
      "[LightGBM] [Info] Start training from score -1.992463\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's binary_logloss: 0.305957\n",
      "Best Threshold (Fold): 0.3000, Best F1: 0.4913\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1122\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
      "[LightGBM] [Info] Start training from score -1.992463\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's binary_logloss: 0.310058\n",
      "Best Threshold (Fold): 0.3000, Best F1: 0.4756\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1123\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
      "[LightGBM] [Info] Start training from score -1.992463\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.303781\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid_0's binary_logloss: 0.303706\n",
      "Best Threshold (Fold): 0.3000, Best F1: 0.4963\n",
      "[LightGBM] [Info] Number of positive: 8368, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1123\n",
      "[LightGBM] [Info] Number of data points in the train set: 69728, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120009 -> initscore=-1.992343\n",
      "[LightGBM] [Info] Start training from score -1.992343\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.309499\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's binary_logloss: 0.308983\n",
      "Best Threshold (Fold): 0.3000, Best F1: 0.4782\n",
      "\n",
      "✅ 평균 Threshold (CV): 0.3000\n",
      "✅ 평균 F1 Score (CV): 0.4846\n",
      "🎉 submission7.csv 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "# 데이터 로딩\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 컬럼 이름 공백 제거\n",
    "train.columns = train.columns.str.strip()\n",
    "test.columns = test.columns.str.strip()\n",
    "\n",
    "# 범주형 변수 라벨 인코딩\n",
    "cat_cols = ['Gender', 'Country', 'Race', 'Family_Background',\n",
    "            'Radiation_History', 'Iodine_Deficiency', 'Smoke',\n",
    "            'Weight_Risk', 'Diabetes']\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train[col], test[col]]))  # train과 test 모두에 적용\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "\n",
    "# 피처와 타겟 분리\n",
    "X = train.drop(['ID', 'Cancer'], axis=1)\n",
    "y = train['Cancer']\n",
    "X_test = test.drop(['ID'], axis=1)\n",
    "\n",
    "# 모델 학습 및 예측\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "test_preds = np.zeros(X_test.shape[0])\n",
    "thresholds = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_idx, valid_idx in skf.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        random_state=1234\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric='binary_logloss',\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=50),\n",
    "            log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_probs = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    # 최적 threshold 찾기\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0.5\n",
    "    for thresh in np.arange(0.3, 0.71, 0.01):\n",
    "        val_preds = (val_probs > thresh).astype(int)\n",
    "        score = f1_score(y_valid, val_preds)\n",
    "        if score > best_f1:\n",
    "            best_f1 = score\n",
    "            best_thresh = thresh\n",
    "\n",
    "    print(f\"Best Threshold (Fold): {best_thresh:.4f}, Best F1: {best_f1:.4f}\")\n",
    "    thresholds.append(best_thresh)\n",
    "    f1_scores.append(best_f1)\n",
    "\n",
    "    test_preds += model.predict_proba(X_test)[:, 1] / skf.n_splits\n",
    "\n",
    "# 최종 threshold 및 예측\n",
    "final_threshold = np.mean(thresholds)\n",
    "print(f\"\\n✅ 평균 Threshold (CV): {final_threshold:.4f}\")\n",
    "print(f\"✅ 평균 F1 Score (CV): {np.mean(f1_scores):.4f}\")\n",
    "\n",
    "final_preds = (test_preds > final_threshold).astype(int)\n",
    "\n",
    "# 제출 파일 저장\n",
    "submission = sample_submission.copy()\n",
    "submission['Cancer'] = final_preds\n",
    "submission.to_csv('submission6.csv', index=False)\n",
    "print(\"🎉 submission7.csv 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13992d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 61360, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 122720, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:44:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003895 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:45:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:45:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:45:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:45:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:45:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold (Fold): 0.5100, Best F1: 0.8902\n",
      "[LightGBM] [Info] Number of positive: 61360, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 122720, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:46:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:46:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:46:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:47:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:47:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:47:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold (Fold): 0.4600, Best F1: 0.8889\n",
      "[LightGBM] [Info] Number of positive: 61360, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 122720, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:47:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:48:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:48:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:48:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:48:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:49:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold (Fold): 0.5000, Best F1: 0.8872\n",
      "[LightGBM] [Info] Number of positive: 61360, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 122720, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:49:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004574 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:50:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:50:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:50:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:50:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:50:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold (Fold): 0.4000, Best F1: 0.8881\n",
      "[LightGBM] [Info] Number of positive: 61360, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 122720, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:51:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 49088, number of negative: 49088\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 98176, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:51:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:51:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:52:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:52:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:52:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold (Fold): 0.4600, Best F1: 0.8894\n",
      "\n",
      "✅ 평균 Threshold (CV): 0.4660\n",
      "✅ 평균 F1 Score (CV): 0.8887\n",
      "🎉 submission8.csv 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 데이터 로딩\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 컬럼 이름 공백 제거\n",
    "train.columns = train.columns.str.strip()\n",
    "test.columns = test.columns.str.strip()\n",
    "\n",
    "# 범주형 변수 라벨 인코딩\n",
    "cat_cols = ['Gender', 'Country', 'Race', 'Family_Background',\n",
    "            'Radiation_History', 'Iodine_Deficiency', 'Smoke',\n",
    "            'Weight_Risk', 'Diabetes']\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train[col], test[col]]))  # train과 test 모두에 적용\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "\n",
    "# 피처와 타겟 분리\n",
    "X = train.drop(['ID', 'Cancer'], axis=1)\n",
    "y = train['Cancer']\n",
    "X_test = test.drop(['ID'], axis=1)\n",
    "\n",
    "# SMOTE 적용\n",
    "smote = SMOTE(random_state=1234)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# 모델 학습 및 예측\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "test_preds = np.zeros(X_test.shape[0])\n",
    "thresholds = []\n",
    "f1_scores = []\n",
    "\n",
    "# 앙상블 모델 정의\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    random_state=1234,\n",
    "    max_depth=10,\n",
    "    num_leaves=50,\n",
    "    subsample=0.8,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    random_state=1234,\n",
    "    max_depth=10,\n",
    "    subsample=0.8,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# 스태킹 모델 정의\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[('lgb', lgb_model), ('xgb', xgb_model)],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "for train_idx, valid_idx in skf.split(X_res, y_res):\n",
    "    X_train, X_valid = X_res.iloc[train_idx], X_res.iloc[valid_idx]\n",
    "    y_train, y_valid = y_res.iloc[train_idx], y_res.iloc[valid_idx]\n",
    "\n",
    "    stacking_model.fit(X_train, y_train)\n",
    "\n",
    "    # 예측 및 최적 threshold 찾기\n",
    "    val_probs = stacking_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0.5\n",
    "    for thresh in np.arange(0.3, 0.71, 0.01):\n",
    "        val_preds = (val_probs > thresh).astype(int)\n",
    "        score = f1_score(y_valid, val_preds)\n",
    "        if score > best_f1:\n",
    "            best_f1 = score\n",
    "            best_thresh = thresh\n",
    "\n",
    "    print(f\"Best Threshold (Fold): {best_thresh:.4f}, Best F1: {best_f1:.4f}\")\n",
    "    thresholds.append(best_thresh)\n",
    "    f1_scores.append(best_f1)\n",
    "\n",
    "    test_preds += stacking_model.predict_proba(X_test)[:, 1] / skf.n_splits\n",
    "\n",
    "# 최종 threshold 및 예측\n",
    "final_threshold = np.mean(thresholds)\n",
    "print(f\"\\n✅ 평균 Threshold (CV): {final_threshold:.4f}\")\n",
    "print(f\"✅ 평균 F1 Score (CV): {np.mean(f1_scores):.4f}\")\n",
    "\n",
    "final_preds = (test_preds > final_threshold).astype(int)\n",
    "\n",
    "# 제출 파일 저장\n",
    "submission = sample_submission.copy()\n",
    "submission['Cancer'] = final_preds\n",
    "submission.to_csv('submission8.csv', index=False)\n",
    "print(\"🎉 submission8.csv 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6cca7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 60674, number of negative: 30924\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 91598, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:49:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 48539, number of negative: 24739\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 73278, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 48539, number of negative: 24739\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 73278, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 48539, number of negative: 24739\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 73278, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 48540, number of negative: 24739\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 73279, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 48539, number of negative: 24740\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1125\n",
      "[LightGBM] [Info] Number of data points in the train set: 73279, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:54:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:54:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:54:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:54:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\302-15\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:55:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# 데이터 로딩\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 컬럼 이름 공백 제거\n",
    "train.columns = train.columns.str.strip()\n",
    "test.columns = test.columns.str.strip()\n",
    "\n",
    "# 범주형 변수 라벨 인코딩\n",
    "cat_cols = ['Gender', 'Country', 'Race', 'Family_Background',\n",
    "            'Radiation_History', 'Iodine_Deficiency', 'Smoke',\n",
    "            'Weight_Risk', 'Diabetes']\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train[col], test[col]]))  # train과 test 모두에 적용\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "\n",
    "# 피처와 타겟 분리\n",
    "X = train.drop(['ID', 'Cancer'], axis=1)\n",
    "y = train['Cancer']\n",
    "X_test = test.drop(['ID'], axis=1)\n",
    "\n",
    "# SMOTE 적용\n",
    "smote_enn = SMOTEENN(random_state=1234)\n",
    "X_res, y_res = smote_enn.fit_resample(X, y)\n",
    "\n",
    "# 모델 학습 및 예측\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "test_preds = np.zeros(X_test.shape[0])\n",
    "thresholds = []\n",
    "f1_scores = []\n",
    "\n",
    "# 앙상블 모델 정의\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    random_state=1234,\n",
    "    max_depth=10,\n",
    "    num_leaves=50,\n",
    "    subsample=0.8,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    random_state=1234,\n",
    "    max_depth=10,\n",
    "    subsample=0.8,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.05, depth=10, random_state=1234, verbose=0)\n",
    "rf_model = RandomForestClassifier(n_estimators=1000, random_state=1234)\n",
    "\n",
    "# 스태킹 모델 정의\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[('lgb', lgb_model), ('xgb', xgb_model), ('catboost', catboost_model), ('rf', rf_model)],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "for train_idx, valid_idx in skf.split(X_res, y_res):\n",
    "    X_train, X_valid = X_res.iloc[train_idx], X_res.iloc[valid_idx]\n",
    "    y_train, y_valid = y_res.iloc[train_idx], y_res.iloc[valid_idx]\n",
    "\n",
    "    stacking_model.fit(X_train, y_train)\n",
    "\n",
    "    # 예측 및 최적 threshold 찾기\n",
    "    val_probs = stacking_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0.5\n",
    "    for thresh in np.arange(0.1, 0.91, 0.05):  # 확장된 threshold 범위\n",
    "        val_preds = (val_probs > thresh).astype(int)\n",
    "        score = f1_score(y_valid, val_preds)\n",
    "        if score > best_f1:\n",
    "            best_f1 = score\n",
    "            best_thresh = thresh\n",
    "\n",
    "    print(f\"Best Threshold (Fold): {best_thresh:.4f}, Best F1: {best_f1:.4f}\")\n",
    "    thresholds.append(best_thresh)\n",
    "    f1_scores.append(best_f1)\n",
    "\n",
    "    test_preds += stacking_model.predict_proba(X_test)[:, 1] / skf.n_splits\n",
    "\n",
    "# 최종 threshold 및 예측\n",
    "final_threshold = np.mean(thresholds)\n",
    "print(f\"\\n✅ 평균 Threshold (CV): {final_threshold:.4f}\")\n",
    "print(f\"✅ 평균 F1 Score (CV): {np.mean(f1_scores):.4f}\")\n",
    "\n",
    "final_preds = (test_preds > final_threshold).astype(int)\n",
    "\n",
    "# 제출 파일 저장\n",
    "submission = sample_submission.copy()\n",
    "submission['Cancer'] = final_preds\n",
    "submission.to_csv('submission_final.csv', index=False)\n",
    "print(\"🎉 submission_final.csv 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17004afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.8-cp312-cp312-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\302-15\\appdata\\roaming\\python\\python312\\site-packages (from catboost) (3.10.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from catboost) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from catboost) (2.1.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from catboost) (1.11.4)\n",
      "Requirement already satisfied: plotly in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\302-15\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.2.3)\n",
      "Downloading catboost-1.2.8-cp312-cp312-win_amd64.whl (102.4 MB)\n",
      "   ---------------------------------------- 0.0/102.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 11.3/102.4 MB 54.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 13.4/102.4 MB 33.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 19.9/102.4 MB 31.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 30.9/102.4 MB 37.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 41.7/102.4 MB 40.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 54.3/102.4 MB 43.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 61.6/102.4 MB 42.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 73.1/102.4 MB 43.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 86.8/102.4 MB 45.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 98.8/102.4 MB 47.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  102.2/102.4 MB 47.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  102.2/102.4 MB 47.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  102.2/102.4 MB 47.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  102.2/102.4 MB 47.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  102.2/102.4 MB 47.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  102.2/102.4 MB 47.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 102.4/102.4 MB 28.4 MB/s eta 0:00:00\n",
      "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz, catboost\n",
      "\n",
      "   -------------------- ------------------- 1/2 [catboost]\n",
      "   -------------------- ------------------- 1/2 [catboost]\n",
      "   -------------------- ------------------- 1/2 [catboost]\n",
      "   -------------------- ------------------- 1/2 [catboost]\n",
      "   -------------------- ------------------- 1/2 [catboost]\n",
      "   -------------------- ------------------- 1/2 [catboost]\n",
      "   -------------------- ------------------- 1/2 [catboost]\n",
      "   -------------------- ------------------- 1/2 [catboost]\n",
      "   -------------------- ------------------- 1/2 [catboost]\n",
      "   -------------------- ------------------- 1/2 [catboost]\n",
      "   -------------------- ------------------- 1/2 [catboost]\n",
      "   -------------------- ------------------- 1/2 [catboost]\n",
      "   -------------------- ------------------- 1/2 [catboost]\n",
      "   ---------------------------------------- 2/2 [catboost]\n",
      "\n",
      "Successfully installed catboost-1.2.8 graphviz-0.20.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\302-15\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\302-15\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\302-15\\anaconda3\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c0a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5c398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ecf9d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1123\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
      "[LightGBM] [Info] Start training from score -1.992463\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.347118\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1123\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
      "[LightGBM] [Info] Start training from score -1.992463\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.346884\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1122\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
      "[LightGBM] [Info] Start training from score -1.992463\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.347474\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1123\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
      "[LightGBM] [Info] Start training from score -1.992463\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.346778\n",
      "[LightGBM] [Info] Number of positive: 8368, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1122\n",
      "[LightGBM] [Info] Number of data points in the train set: 69728, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120009 -> initscore=-1.992343\n",
      "[LightGBM] [Info] Start training from score -1.992343\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.346873\n",
      "\n",
      "✅ 최적 Threshold (전체 기준): 0.2200\n",
      "✅ 전체 Validation F1 Score: 0.4865\n",
      "🎉 submission9.csv 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "# 데이터 로딩\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 컬럼 공백 제거\n",
    "train.columns = train.columns.str.strip()\n",
    "test.columns = test.columns.str.strip()\n",
    "\n",
    "# 범주형 변수 라벨 인코딩\n",
    "cat_cols = ['Gender', 'Country', 'Race', 'Family_Background',\n",
    "            'Radiation_History', 'Iodine_Deficiency', 'Smoke',\n",
    "            'Weight_Risk', 'Diabetes']\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train[col], test[col]]))\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "\n",
    "# 피처와 타겟 분리\n",
    "X = train.drop(['ID', 'Cancer'], axis=1)\n",
    "y = train['Cancer']\n",
    "X_test = test.drop(['ID'], axis=1)\n",
    "\n",
    "# 교차검증 설정\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "test_preds = np.zeros(X_test.shape[0])\n",
    "val_probs_all = []\n",
    "val_targets_all = []\n",
    "\n",
    "# 모델 학습\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "    # 클래스 불균형 가중치 계산\n",
    "    scale_pos_weight = 5  # 불균형 비율에 따른 적당한 가중치\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric='binary_logloss',\n",
    "        callbacks=[\n",
    "            early_stopping(50),\n",
    "            log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_probs = model.predict_proba(X_valid)[:, 1]\n",
    "    val_probs_all.extend(val_probs)\n",
    "    val_targets_all.extend(y_valid)\n",
    "\n",
    "    test_preds += model.predict_proba(X_test)[:, 1] / skf.n_splits\n",
    "\n",
    "# 최적 threshold 탐색\n",
    "val_probs_all = np.array(val_probs_all)\n",
    "val_targets_all = np.array(val_targets_all)\n",
    "best_thresh = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for thresh in np.arange(0.0, 1.0, 0.01):\n",
    "    preds = (val_probs_all > thresh).astype(int)\n",
    "    if preds.sum() == 0:  # 모두 0이면 F1 score 계산 불가\n",
    "        continue\n",
    "    score = f1_score(val_targets_all, preds)\n",
    "    if score > best_f1:\n",
    "        best_f1 = score\n",
    "        best_thresh = thresh\n",
    "\n",
    "# 최종 예측\n",
    "final_preds = (test_preds > best_thresh).astype(int)\n",
    "\n",
    "print(f\"\\n✅ 최적 Threshold (전체 기준): {best_thresh:.4f}\")\n",
    "print(f\"✅ 전체 Validation F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "# 제출 파일 저장\n",
    "submission = sample_submission.copy()\n",
    "submission['Cancer'] = final_preds\n",
    "submission.to_csv('submission9.csv', index=False)\n",
    "print(\"🎉 submission9.csv 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "687ce765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1127\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
      "[LightGBM] [Info] Start training from score -1.992463\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's binary_logloss: 0.309605\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's binary_logloss: 0.308824\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1126\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
      "[LightGBM] [Info] Start training from score -1.992463\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's binary_logloss: 0.307543\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid_0's binary_logloss: 0.307317\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1126\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
      "[LightGBM] [Info] Start training from score -1.992463\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.311931\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid_0's binary_logloss: 0.311165\n",
      "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1127\n",
      "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
      "[LightGBM] [Info] Start training from score -1.992463\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's binary_logloss: 0.304588\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid_0's binary_logloss: 0.304502\n",
      "[LightGBM] [Info] Number of positive: 8368, number of negative: 61360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1127\n",
      "[LightGBM] [Info] Number of data points in the train set: 69728, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120009 -> initscore=-1.992343\n",
      "[LightGBM] [Info] Start training from score -1.992343\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's binary_logloss: 0.310788\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's binary_logloss: 0.310352\n",
      "\n",
      "✅ 최적 Threshold (전체 기준): 0.2200\n",
      "✅ 전체 Validation F1 Score: 0.4866\n",
      "🎉 submission10.csv 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "# 데이터 로딩\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 컬럼 이름 공백 제거\n",
    "train.columns = train.columns.str.strip()\n",
    "test.columns = test.columns.str.strip()\n",
    "\n",
    "# 상호작용 피처 및 전처리 함수 정의\n",
    "def feature_engineering(df):\n",
    "    # Smoke와 Gender 상호작용\n",
    "    df['Smoke_Gender'] = df['Smoke'].astype(str) + '_' + df['Gender'].astype(str)\n",
    "    return df\n",
    "\n",
    "train = feature_engineering(train)\n",
    "test = feature_engineering(test)\n",
    "\n",
    "# 범주형 변수 라벨 인코딩 및 희소 그룹 통합\n",
    "cat_cols = ['Gender', 'Country', 'Race', 'Family_Background',\n",
    "            'Radiation_History', 'Iodine_Deficiency', 'Smoke',\n",
    "            'Weight_Risk', 'Diabetes', 'Smoke_Gender']\n",
    "\n",
    "for col in cat_cols:\n",
    "    # 희소 클래스 통합 (10개 이하로 등장한 값은 'Rare'로 치환)\n",
    "    value_counts = train[col].value_counts()\n",
    "    rare_classes = value_counts[value_counts < 10].index\n",
    "    train[col] = train[col].replace(rare_classes, 'Rare')\n",
    "    test[col] = test[col].replace(rare_classes, 'Rare')\n",
    "\n",
    "    # 라벨 인코딩\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train[col], test[col]]))\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "\n",
    "# 피처와 타겟 분리\n",
    "X = train.drop(['ID', 'Cancer'], axis=1)\n",
    "y = train['Cancer']\n",
    "X_test = test.drop(['ID'], axis=1)\n",
    "\n",
    "# 모델 학습 및 예측\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "test_preds = np.zeros(X_test.shape[0])\n",
    "val_probs_all = []\n",
    "y_valid_all = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        num_leaves=64,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=1234,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric='binary_logloss',\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=50),\n",
    "            log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_probs = model.predict_proba(X_valid)[:, 1]\n",
    "    val_probs_all.extend(val_probs)\n",
    "    y_valid_all.extend(y_valid)\n",
    "\n",
    "    test_preds += model.predict_proba(X_test)[:, 1] / skf.n_splits\n",
    "\n",
    "# 최적 threshold 전체 기준으로 계산\n",
    "val_probs_all = np.array(val_probs_all)\n",
    "y_valid_all = np.array(y_valid_all)\n",
    "\n",
    "best_f1 = 0\n",
    "best_thresh = 0.5\n",
    "for thresh in np.arange(0.1, 0.71, 0.01):\n",
    "    val_preds_bin = (val_probs_all > thresh).astype(int)\n",
    "    score = f1_score(y_valid_all, val_preds_bin)\n",
    "    if score > best_f1:\n",
    "        best_f1 = score\n",
    "        best_thresh = thresh\n",
    "\n",
    "print(f\"\\n✅ 최적 Threshold (전체 기준): {best_thresh:.4f}\")\n",
    "print(f\"✅ 전체 Validation F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "# 최종 예측 및 저장\n",
    "final_preds = (test_preds > best_thresh).astype(int)\n",
    "submission = sample_submission.copy()\n",
    "submission['Cancer'] = final_preds\n",
    "submission.to_csv('submission9.csv', index=False)\n",
    "print(\"🎉 submission10.csv 저장 완료\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
